{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 5.1 - Word2Vec\n",
    "\n",
    "В этом задании мы натренируем свои word vectors на очень небольшом датасете.\n",
    "Мы будем использовать самую простую версию word2vec, без negative sampling и других оптимизаций.\n",
    "\n",
    "Перед запуском нужно запустить скрипт `download_data.sh` чтобы скачать данные.\n",
    "\n",
    "Датасет и модель очень небольшие, поэтому это задание можно выполнить и без GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We'll use Principal Component Analysis (PCA) to visualize word vectors,\n",
    "# so make sure you install dependencies from requirements.txt!\n",
    "from sklearn.decomposition import PCA \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num tokens: 19538\n",
      "marred ['nuance', 'inexplicable', 'blunder']\n",
      "three ['memory', 'lane', 'none', 'above']\n",
      "effects ['action', 'special']\n",
      "in ['package', 'wrapped', 'disappointingly']\n",
      "right ['brilliant', 'piece']\n",
      "Wall time: 220 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class StanfordTreeBank:\n",
    "    '''\n",
    "    Wrapper for accessing Stanford Tree Bank Dataset\n",
    "    https://nlp.stanford.edu/sentiment/treebank.html\n",
    "    \n",
    "    Parses dataset, gives each token and index and provides lookups\n",
    "    from string token to index and back\n",
    "    \n",
    "    Allows to generate random context with sampling strategy described in\n",
    "    word2vec paper:\n",
    "    https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.index_by_token = {}\n",
    "        self.token_by_index = []\n",
    "\n",
    "        self.sentences = []\n",
    "\n",
    "        self.token_freq = {}\n",
    "        \n",
    "        self.token_reject_by_index = None\n",
    "\n",
    "    def load_dataset(self, folder):\n",
    "        filename = os.path.join(folder, \"datasetSentences.txt\")\n",
    "\n",
    "        with open(filename, \"r\", encoding=\"latin1\") as f:\n",
    "            l = f.readline() # skip the first line\n",
    "            \n",
    "            for l in f:\n",
    "                splitted_line = l.strip().split()\n",
    "                words = [w.lower() for w in splitted_line[1:]] # First one is a number\n",
    "                    \n",
    "                self.sentences.append(words)\n",
    "                for word in words:\n",
    "                    if word in self.token_freq:\n",
    "                        self.token_freq[word] +=1 \n",
    "                    else:\n",
    "                        index = len(self.token_by_index)\n",
    "                        self.token_freq[word] = 1\n",
    "                        self.index_by_token[word] = index\n",
    "                        self.token_by_index.append(word)\n",
    "        self.compute_token_prob()\n",
    "                        \n",
    "    def compute_token_prob(self):\n",
    "        words_count = np.array([self.token_freq[token] for token in self.token_by_index])\n",
    "        words_freq = words_count / np.sum(words_count)\n",
    "        \n",
    "        # Following sampling strategy from word2vec paper:\n",
    "        # https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\n",
    "        self.token_reject_by_index = 1- np.sqrt(1e-5/words_freq)\n",
    "    \n",
    "    def check_reject(self, word):\n",
    "        return np.random.rand() > self.token_reject_by_index[self.index_by_token[word]]\n",
    "        \n",
    "    def get_random_context(self, context_length=5):\n",
    "        \"\"\"\n",
    "        Returns tuple of center word and list of context words\n",
    "        \"\"\"\n",
    "        sentence_sampled = []\n",
    "        while len(sentence_sampled) <= 2:\n",
    "            sentence_index = np.random.randint(len(self.sentences)) \n",
    "            sentence = self.sentences[sentence_index]\n",
    "            sentence_sampled = [word for word in sentence if self.check_reject(word)]\n",
    "    \n",
    "        center_word_index = np.random.randint(len(sentence_sampled))\n",
    "        \n",
    "        words_before = sentence_sampled[max(center_word_index - context_length//2,0):center_word_index]\n",
    "        words_after = sentence_sampled[center_word_index+1: center_word_index+1+context_length//2]\n",
    "        \n",
    "        return sentence_sampled[center_word_index], words_before+words_after\n",
    "    \n",
    "    def num_tokens(self):\n",
    "        return len(self.token_by_index)\n",
    "        \n",
    "data = StanfordTreeBank()\n",
    "data.load_dataset(\"./stanfordSentimentTreebank/\")\n",
    "\n",
    "print(\"Num tokens:\", data.num_tokens())\n",
    "for i in range(5):\n",
    "    center_word, other_words = data.get_random_context(5)\n",
    "    print(center_word, other_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Имплеменируем PyTorch-style Dataset для Word2Vec\n",
    "\n",
    "Этот Dataset должен сгенерировать много случайных контекстов и превратить их в сэмплы для тренировки.\n",
    "\n",
    "Напоминаем, что word2vec модель получает на вход One-hot вектор слова и тренирует простую сеть для предсказания на его основе соседних слов.\n",
    "Из набора слово-контекст создается N сэмплов (где N - количество слов в контексте):\n",
    "\n",
    "Например:\n",
    "\n",
    "Слово: `orders` и контекст: `['love', 'nicest', 'to', '50-year']` создадут 4 сэмпла:\n",
    "- input: `orders`, target: `love`\n",
    "- input: `orders`, target: `nicest`\n",
    "- input: `orders`, target: `to`\n",
    "- input: `orders`, target: `50-year`\n",
    "\n",
    "Все слова на входе и на выходе закодированы через one-hot encoding, с размером вектора равным количеству токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample - input: tensor([0, 0, 0,  ..., 0, 0, 0]), target: 13306\n",
      "Wall time: 2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "class Word2VecPlain(Dataset):\n",
    "    '''\n",
    "    PyTorch Dataset for plain Word2Vec.\n",
    "    Accepts StanfordTreebank as data and is able to generate dataset based on\n",
    "    a number of random contexts\n",
    "    '''\n",
    "    def __init__(self, data, num_contexts=30000):\n",
    "        '''\n",
    "        Initializes Word2VecPlain, but doesn't generate the samples yet\n",
    "        (for that, use generate_dataset)\n",
    "        Arguments:\n",
    "        data - StanfordTreebank instace\n",
    "        num_contexts - number of random contexts to use when generating a dataset\n",
    "        '''\n",
    "        # TODO: Implement what you need for other methods!\n",
    "        self.data = data\n",
    "        self.num_contexts = num_contexts\n",
    "        self.num_tokens = data.num_tokens()\n",
    "    \n",
    "    def generate_dataset(self):\n",
    "        '''\n",
    "        Generates dataset samples from random contexts\n",
    "        Note: there will be more samples than contexts because every context\n",
    "        can generate more than one sample\n",
    "        '''\n",
    "        # TODO: Implement generating the dataset\n",
    "        # You should sample num_contexts contexts from the data and turn them into samples\n",
    "        # Note you will have several samples from one context\n",
    "        self.dataset = []\n",
    "        for i in range(self.num_contexts):\n",
    "            center_word, others_words = self.data.get_random_context(self.num_contexts)\n",
    "            for j in range(len(others_words)):\n",
    "                self.dataset.append([center_word, others_words[j]])\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Returns total number of samples\n",
    "        '''\n",
    "        # TODO: Return the number of samples\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        Returns i-th sample\n",
    "        \n",
    "        Return values:\n",
    "        input_vector - torch.Tensor with one-hot representation of the input vector\n",
    "        output_index - index of the target word (not torch.Tensor!)\n",
    "        '''\n",
    "        # TODO: Generate tuple of 2 return arguments for i-th sample\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.to_list()\n",
    "        input_word, output_word = self.dataset[index]\n",
    "        \n",
    "        input_index = self.data.index_by_token[input_word]\n",
    "        input_vector = torch.nn.functional.one_hot(torch.tensor(input_index), self.data.num_tokens()) \n",
    "        output_index = self.data.index_by_token[output_word]\n",
    "        \n",
    "        return input_vector, output_index\n",
    "\n",
    "dataset = Word2VecPlain(data, 10)\n",
    "dataset.generate_dataset()\n",
    "input_vector, target = dataset[3]\n",
    "print(\"Sample - input: %s, target: %s\" % (input_vector, int(target))) # target should be able to convert to int\n",
    "assert isinstance(input_vector, torch.Tensor)\n",
    "assert torch.sum(input_vector) == 1.0\n",
    "assert input_vector.shape[0] == data.num_tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создаем модель и тренируем ее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.57 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=19538, out_features=10, bias=False)\n",
       "  (1): Linear(in_features=10, out_features=19538, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Create the usual PyTorch structures\n",
    "dataset = Word2VecPlain(data, 30000)\n",
    "dataset.generate_dataset()\n",
    "\n",
    "# We'll be training very small word vectors!\n",
    "wordvec_dim = 10\n",
    "\n",
    "# We can use a standard sequential model for this\n",
    "nn_model = nn.Sequential(\n",
    "            nn.Linear(dataset.num_tokens, wordvec_dim, bias=False),\n",
    "            nn.Linear(wordvec_dim, dataset.num_tokens, bias=False), \n",
    "         )\n",
    "nn_model.type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def extract_word_vectors(nn_model):\n",
    "    '''\n",
    "    Extracts word vectors from the model\n",
    "    \n",
    "    Returns:\n",
    "    input_vectors: torch.Tensor with dimensions (num_tokens, num_dimensions)\n",
    "    output_vectors: torch.Tensor with dimensions (num_tokens, num_dimensions)\n",
    "    '''\n",
    "    # TODO: Implement extracting word vectors from param weights\n",
    "    # return tuple of input vectors and output vectos \n",
    "    # Hint: you can access weights as Tensors through nn.Linear class attributes\n",
    "    input_vectors = nn_model[0].weight.t()\n",
    "    output_vectors = nn_model[1].weight\n",
    "    return input_vectors, output_vectors\n",
    "\n",
    "untrained_input_vectors, untrained_output_vectors = extract_word_vectors(nn_model)\n",
    "assert untrained_input_vectors.shape == (data.num_tokens(), wordvec_dim)\n",
    "assert untrained_output_vectors.shape == (data.num_tokens(), wordvec_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def train_model(model, dataset, train_loader, optimizer, scheduler, num_epochs):\n",
    "    '''\n",
    "    Trains plain word2vec using cross-entropy loss and regenerating dataset every epoch\n",
    "    \n",
    "    Returns:\n",
    "    loss_history, train_history\n",
    "    '''\n",
    "    \n",
    "    loss = nn.CrossEntropyLoss().type(torch.FloatTensor)\n",
    "    \n",
    "    loss_history = []\n",
    "    train_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # Enter train mode\n",
    "                \n",
    "        dataset.generate_dataset() # Regenerate dataset every epoch\n",
    "        \n",
    "        # TODO Implement training for this model\n",
    "        # Note we don't have any validation set here because our purpose is the word vectors,\n",
    "        # not the predictive performance of the model\n",
    "        #\n",
    "        # And don't forget to step the learing rate scheduler! \n",
    "        train_loader = torch.utils.data.DataLoader(dataset, batch_size=20)\n",
    "        loss_accum = 0\n",
    "        correct_samples = 0\n",
    "        total_samples = 0\n",
    "        for i_step, (x, y) in enumerate(train_loader):  \n",
    "            \n",
    "            prediction = model(x.float())\n",
    "            loss_value = loss(prediction, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            _, indices = torch.max(prediction, 1)\n",
    "            correct_samples += torch.sum(indices == y)\n",
    "            total_samples += y.shape[0]\n",
    "            \n",
    "            loss_accum += loss_value\n",
    "\n",
    "        ave_loss = loss_accum / i_step\n",
    "        train_accuracy = float(correct_samples) / total_samples\n",
    "        \n",
    "        loss_history.append(float(ave_loss))\n",
    "        train_history.append(train_accuracy)\n",
    "        \n",
    "        print(\"Epoch %i, Average loss: %f, Train accuracy: %f\" % (epoch, ave_loss, train_accuracy))\n",
    "        \n",
    "    return loss_history, train_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну и наконец тренировка!\n",
    "\n",
    "Добейтесь значения ошибки меньше **8.0**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Average loss: 9.881275, Train accuracy: 0.000028\n",
      "Epoch 1, Average loss: 9.881298, Train accuracy: 0.000028\n",
      "Epoch 2, Average loss: 9.881303, Train accuracy: 0.000050\n",
      "Epoch 3, Average loss: 9.881294, Train accuracy: 0.000078\n",
      "Epoch 4, Average loss: 9.881289, Train accuracy: 0.000064\n",
      "Epoch 5, Average loss: 9.881272, Train accuracy: 0.000071\n",
      "Epoch 6, Average loss: 9.881273, Train accuracy: 0.000042\n",
      "Epoch 7, Average loss: 9.881277, Train accuracy: 0.000035\n",
      "Epoch 8, Average loss: 9.881289, Train accuracy: 0.000014\n",
      "Epoch 9, Average loss: 9.881278, Train accuracy: 0.000071\n",
      "Wall time: 20min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Finally, let's train the model!\n",
    "\n",
    "# TODO: We use placeholder values for hyperparameters - you will need to find better values!\n",
    "optimizer = optim.SGD(nn_model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.2)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=20)\n",
    "\n",
    "loss_history, train_history = train_model(nn_model, dataset, train_loader, optimizer, scheduler, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0xf3c0910>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize training graphs\n",
    "plt.subplot(211)\n",
    "plt.plot(train_history)\n",
    "plt.subplot(212)\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Визуализируем вектора для разного вида слов до и после тренировки\n",
    "\n",
    "В случае успешной тренировки вы должны увидеть как вектора слов разных типов (например, знаков препинания, предлогов и остальных) разделяются семантически.\n",
    "\n",
    "Студенты - в качестве выполненного задания присылайте notebook с диаграммами!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36mvisualize_vectors\u001b[1;34m(input_vectors, output_vectors, title)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_pca.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[0mC\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mordered\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse\u001b[0m \u001b[1;34m'np.ascontiguousarray'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m         \"\"\"\n\u001b[1;32m--> 376\u001b[1;33m         \u001b[0mU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    377\u001b[0m         \u001b[0mU\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mU\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_components_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_pca.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    395\u001b[0m                             'TruncatedSVD for a possible alternative.')\n\u001b[0;32m    396\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m         X = self._validate_data(X, dtype=[np.float64, np.float32],\n\u001b[0m\u001b[0;32m    398\u001b[0m                                 ensure_2d=True, copy=self.copy)\n\u001b[0;32m    399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    418\u001b[0m                     \u001b[1;34mf\"requires y to be passed, but the target y is None.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m                 )\n\u001b[1;32m--> 420\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    421\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    597\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    480\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;31m# Wrap Numpy array again in a suitable tensor when done, to support e.g.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trained_input_vectors, trained_output_vectors = extract_word_vectors(nn_model)\n",
    "assert trained_input_vectors.shape == (data.num_tokens(), wordvec_dim)\n",
    "assert trained_output_vectors.shape == (data.num_tokens(), wordvec_dim)\n",
    "\n",
    "def visualize_vectors(input_vectors, output_vectors, title=''):\n",
    "    i =42\n",
    "    full_vectors = torch.cat((input_vectors, output_vectors), 0)\n",
    "    wordvec_embedding = PCA(n_components=2).fit_transform(full_vectors)\n",
    "\n",
    "    # Helpful words form CS244D example\n",
    "    # http://cs224d.stanford.edu/assignment1/index.html\n",
    "    visualize_words = {'green': [\"the\", \"a\", \"an\"], \n",
    "                      'blue': [\",\", \".\", \"?\", \"!\", \"``\", \"''\", \"--\"], \n",
    "                      'brown': [\"good\", \"great\", \"cool\", \"brilliant\", \"wonderful\", \n",
    "                              \"well\", \"amazing\", \"worth\", \"sweet\", \"enjoyable\"],\n",
    "                      'orange': [\"boring\", \"bad\", \"waste\", \"dumb\", \"annoying\", \"stupid\"],\n",
    "                      'red': ['tell', 'told', 'said', 'say', 'says', 'tells', 'goes', 'go', 'went']\n",
    "                     }\n",
    "\n",
    "#     plt.figure(figsize=(7,7))\n",
    "#     plt.suptitle(title)\n",
    "#     for color, words in visualize_words.items():\n",
    "#         points = np.array([wordvec_embedding[data.index_by_token[w]] for w in words])\n",
    "#         for i, word in enumerate(words):\n",
    "#             plt.text(points[i, 0], points[i, 1], word, color=color,horizontalalignment='center')\n",
    "#         plt.scatter(points[:, 0], points[:, 1], c=color, alpha=0.3, s=0.5)\n",
    "\n",
    "visualize_vectors(untrained_input_vectors, untrained_output_vectors, \"Untrained word vectors\")\n",
    "visualize_vectors(trained_input_vectors, trained_output_vectors, \"Trained word vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
