{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.1 - Нейронные сети\n",
    "\n",
    "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
    "\n",
    "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
    "\n",
    "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from layers import FullyConnectedLayer, ReLULayer\n",
    "from model import TwoLayerNet\n",
    "from trainer import Trainer, Dataset\n",
    "from optim import SGD, MomentumSGD\n",
    "from metrics import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "\n",
    "И разделяем их на training и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analytic_grad.shape: (2, 3)\n",
      "x.shape: (2, 3)\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# DONE: Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "\n",
    "# ReLULayer().forward(X)\n",
    "\n",
    "assert check_layer_gradient(ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
    "\n",
    "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
    "\n",
    "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analytic_grad.shape: (2, 3)\n",
      "x.shape: (2, 3)\n",
      "Gradient check passed!\n",
      "analytic_grad.shape: (3, 4)\n",
      "x.shape: (3, 4)\n",
      "Gradient check passed!\n",
      "analytic_grad.shape: (1, 4)\n",
      "x.shape: (1, 4)\n",
      "Gradient check passed!\n",
      "Wall time: 265 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# DONE: Implement FullyConnected layer forward and backward methods\n",
    "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
    "# DONE: Implement storing gradients for W and B\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем нейронную сеть\n",
    "\n",
    "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
    "\n",
    "Не забудьте реализовать очистку градиентов в начале функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for first_layer.W\n",
      "analytic_grad.shape: (3072, 3)\n",
      "x.shape: (3072, 3)\n",
      "Gradient check passed!\n",
      "Checking gradient for first_layer.B\n",
      "analytic_grad.shape: (1, 3)\n",
      "x.shape: (1, 3)\n",
      "Gradient check passed!\n",
      "Checking gradient for second_layer.W\n",
      "analytic_grad.shape: (3, 10)\n",
      "x.shape: (3, 10)\n",
      "Gradient check passed!\n",
      "Checking gradient for second_layer.B\n",
      "analytic_grad.shape: (1, 10)\n",
      "x.shape: (1, 10)\n",
      "Gradient check passed!\n",
      "Wall time: 55.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# TODO: In model.py, implement compute_loss_and_gradients function\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "\n",
    "# TODO Now implement backward pass and aggregate all of the params\n",
    "check_model_gradient(model, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.394757354752797\n",
      "Checking gradient for first_layer.W\n",
      "analytic_grad.shape: (3072, 3)\n",
      "x.shape: (3072, 3)\n",
      "Gradient check passed!\n",
      "Checking gradient for first_layer.B\n",
      "analytic_grad.shape: (1, 3)\n",
      "x.shape: (1, 3)\n",
      "Gradient check passed!\n",
      "Checking gradient for second_layer.W\n",
      "analytic_grad.shape: (3, 10)\n",
      "x.shape: (3, 10)\n",
      "Gradient check passed!\n",
      "Checking gradient for second_layer.B\n",
      "analytic_grad.shape: (1, 10)\n",
      "x.shape: (1, 10)\n",
      "Gradient check passed!\n",
      "Wall time: 15.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# TODO Now implement l2 regularization in the forward and backward pass\n",
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "print(loss_with_reg)\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
    "\n",
    "check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
    "\n",
    "Какое значение точности мы ожидаем увидеть до начала тренировки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Finally, implement predict function!\n",
    "\n",
    "# DONE: Implement predict function\n",
    "# What would be the value we expect?\n",
    "# print(train_X[:30])\n",
    "multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишем код для процесса тренировки\n",
    "\n",
    "Если все реализовано корректно, значение функции ошибки должно уменьшаться с каждой эпохой, пусть и медленно. Не беспокойтесь пока про validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.301730, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.300799, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301776, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.303332, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301541, Train accuracy: 0.148222, val accuracy: 0.140000\n",
      "Loss: 2.301563, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301714, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301334, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302613, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302540, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302082, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302348, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.303074, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302038, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301894, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301366, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302119, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302083, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301297, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302160, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Wall time: 9min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate = 1e-2)\n",
    "\n",
    "# FIXME seems the issue on the layers layer (runtime errors, NaN as loss result)\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down every epoch, even if it's slow\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x5ae4c70>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшаем процесс тренировки\n",
    "\n",
    "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уменьшение скорости обучения (learning rate decay)\n",
    "\n",
    "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
    "\n",
    "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
    "\n",
    "В нашем случае N будет равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.243613, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.298593, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.236525, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.212541, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.336710, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.237617, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.291851, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.247074, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.271449, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.266921, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.272612, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.224796, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.263617, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.298726, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.221359, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.303228, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.319364, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.346858, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.220475, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.292955, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Wall time: 2min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# DONE Implement learning rate decay inside Trainer.fit method\n",
    "# Decay should happen once per epoch\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate_decay=0.99)\n",
    "\n",
    "initial_learning_rate = trainer.learning_rate\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
    "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Накопление импульса (Momentum SGD)\n",
    "\n",
    "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
    "\n",
    "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
    "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
    "\n",
    "```\n",
    "velocity = momentum * velocity - learning_rate * gradient \n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
    "\n",
    "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
    "http://cs231n.github.io/neural-networks-3/#sgd  \n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.319482, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.306641, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.313926, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.313440, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.298050, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.265974, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302546, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.311003, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.334133, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.332669, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.281388, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.332419, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.295786, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.297263, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.261716, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.324117, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.288974, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.272671, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.251290, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.234552, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Wall time: 2min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# TODO: Implement MomentumSGD.update function in optim.py\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-4, learning_rate_decay=0.99)\n",
    "\n",
    "# You should see even better results than before!\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну что, давайте уже тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
    "\n",
    "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
    "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
    "\n",
    "Если этого не происходит, то где-то была допущена ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.339444, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.312066, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.317865, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.323978, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.277234, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.240588, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.293525, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.348892, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.250476, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.110080, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.158428, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.978957, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.183809, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.475926, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.330693, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.902110, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.030481, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.135387, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.632803, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.997377, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.116618, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.577501, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.342602, Train accuracy: 0.400000, val accuracy: 0.066667\n",
      "Loss: 1.915818, Train accuracy: 0.400000, val accuracy: 0.066667\n",
      "Loss: 1.776445, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.326349, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.280094, Train accuracy: 0.400000, val accuracy: 0.066667\n",
      "Loss: 1.218437, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.851130, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.705378, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.131983, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 1.635010, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 2.109653, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.969246, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.650840, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 2.033358, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.587459, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.056531, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.713991, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.737742, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.636943, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.885087, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.664881, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.799904, Train accuracy: 0.666667, val accuracy: 0.133333\n",
      "Loss: 0.949699, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.428387, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.614644, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.906414, Train accuracy: 0.733333, val accuracy: 0.133333\n",
      "Loss: 1.219015, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.521772, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.494944, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.319311, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.639918, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.476791, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.718600, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.489677, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.181399, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.041556, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.283690, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.824714, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.359035, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.614395, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.596687, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.353587, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 2.020191, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.375956, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 2.011360, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.607484, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.607932, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.691064, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.616778, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.331352, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.379066, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.232669, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.357323, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.328210, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.458794, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.261228, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.269219, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.959376, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.584070, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.897449, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.598947, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.682210, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.281119, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.251078, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.561577, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.259313, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.504664, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.532161, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.579963, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.533868, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.077958, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 0.967841, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.614174, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.572543, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.067023, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 0.990372, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.515819, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 0.988079, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.431975, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.323251, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.366172, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.611376, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.230167, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.610601, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.233376, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.503761, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.751648, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.360724, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.468304, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.280939, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.087435, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.115355, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.326569, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.369959, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.391751, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.226221, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.454094, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.476086, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.416705, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.278328, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.277106, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.101545, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.324465, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.110012, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.625384, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.105079, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.492194, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.132987, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.208204, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.337123, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.066690, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.179330, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.506226, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.116357, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.598203, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.166217, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.169326, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.436986, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.223732, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.193639, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.230147, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.496435, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.250413, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.223684, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.326787, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.279634, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.428066, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.264153, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Wall time: 6.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_size = 15\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=150, batch_size=5)\n",
    "\n",
    "# You should expect this to reach 1.0 training accuracy \n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
    "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
    "Найдите их!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.331845, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 2.335453, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.296374, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.301203, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.265853, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.233992, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.317392, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.230214, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.265169, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.101961, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.731350, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.706587, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.630759, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.934180, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.032505, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.608217, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.336355, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.981169, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.699087, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.042932, Train accuracy: 0.400000, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=7e-1, num_epochs=20, batch_size=5)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итак, основное мероприятие!\n",
    "\n",
    "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
    "\n",
    "Добейтесь точности лучше **60%** на validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "must be real number, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-167b1f217e7a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# Save loss/train/history of the best classifier to the variables above\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best validation accuracy achieved: %f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbest_val_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: must be real number, not NoneType"
     ]
    }
   ],
   "source": [
    "# Let's train the best one-hidden-layer network we can\n",
    "\n",
    "learning_rates = 1e-4\n",
    "reg_strength = 1e-3\n",
    "learning_rate_decay = 0.999\n",
    "hidden_layer_size = 128\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "loss_history = []\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "# TODO find the best hyperparameters to train the network\n",
    "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
    "# You should expect to get to at least 40% of valudation accuracy\n",
    "# Save loss/train/history of the best classifier to the variables above\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.subplot(212)\n",
    "plt.title(\"Train/validation accuracy\")\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
