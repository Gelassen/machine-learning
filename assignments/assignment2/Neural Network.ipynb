{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.1 - Нейронные сети\n",
    "\n",
    "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
    "\n",
    "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
    "\n",
    "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from layers import FullyConnectedLayer, ReLULayer\n",
    "from model import TwoLayerNet\n",
    "from trainer import Trainer, Dataset\n",
    "from optim import SGD, MomentumSGD\n",
    "from metrics import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "\n",
    "И разделяем их на training и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analytic_grad.shape: (2, 3)\n",
      "x.shape: (2, 3)\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# DONE: Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "\n",
    "# ReLULayer().forward(X)\n",
    "\n",
    "assert check_layer_gradient(ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
    "\n",
    "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
    "\n",
    "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analytic_grad.shape: (2, 3)\n",
      "x.shape: (2, 3)\n",
      "Gradient check passed!\n",
      "analytic_grad.shape: (3, 4)\n",
      "x.shape: (3, 4)\n",
      "Gradient check passed!\n",
      "analytic_grad.shape: (1, 4)\n",
      "x.shape: (1, 4)\n",
      "Gradient check passed!\n",
      "Wall time: 265 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# DONE: Implement FullyConnected layer forward and backward methods\n",
    "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
    "# DONE: Implement storing gradients for W and B\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем нейронную сеть\n",
    "\n",
    "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
    "\n",
    "Не забудьте реализовать очистку градиентов в начале функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for first_layer.W\n",
      "analytic_grad.shape: (3072, 3)\n",
      "x.shape: (3072, 3)\n",
      "Gradient check passed!\n",
      "Checking gradient for first_layer.B\n",
      "analytic_grad.shape: (1, 3)\n",
      "x.shape: (1, 3)\n",
      "Gradient check passed!\n",
      "Checking gradient for second_layer.W\n",
      "analytic_grad.shape: (3, 10)\n",
      "x.shape: (3, 10)\n",
      "Gradient check passed!\n",
      "Checking gradient for second_layer.B\n",
      "analytic_grad.shape: (1, 10)\n",
      "x.shape: (1, 10)\n",
      "Gradient check passed!\n",
      "Wall time: 55.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# TODO: In model.py, implement compute_loss_and_gradients function\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "\n",
    "# TODO Now implement backward pass and aggregate all of the params\n",
    "check_model_gradient(model, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.394757354752797\n",
      "Checking gradient for first_layer.W\n",
      "analytic_grad.shape: (3072, 3)\n",
      "x.shape: (3072, 3)\n",
      "Gradient check passed!\n",
      "Checking gradient for first_layer.B\n",
      "analytic_grad.shape: (1, 3)\n",
      "x.shape: (1, 3)\n",
      "Gradient check passed!\n",
      "Checking gradient for second_layer.W\n",
      "analytic_grad.shape: (3, 10)\n",
      "x.shape: (3, 10)\n",
      "Gradient check passed!\n",
      "Checking gradient for second_layer.B\n",
      "analytic_grad.shape: (1, 10)\n",
      "x.shape: (1, 10)\n",
      "Gradient check passed!\n",
      "Wall time: 15.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# TODO Now implement l2 regularization in the forward and backward pass\n",
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "print(loss_with_reg)\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
    "\n",
    "check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
    "\n",
    "Какое значение точности мы ожидаем увидеть до начала тренировки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Finally, implement predict function!\n",
    "\n",
    "# DONE: Implement predict function\n",
    "# What would be the value we expect?\n",
    "# print(train_X[:30])\n",
    "multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишем код для процесса тренировки\n",
    "\n",
    "Если все реализовано корректно, значение функции ошибки должно уменьшаться с каждой эпохой, пусть и медленно. Не беспокойтесь пока про validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.301730, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.300799, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301776, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.303332, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301541, Train accuracy: 0.148222, val accuracy: 0.140000\n",
      "Loss: 2.301563, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301714, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301334, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302613, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302540, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302082, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302348, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.303074, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302038, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301894, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301366, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302119, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302083, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301297, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302160, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Wall time: 9min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate = 1e-2)\n",
    "\n",
    "# FIXME seems the issue on the layers layer (runtime errors, NaN as loss result)\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down every epoch, even if it's slow\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x5ae4c70>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшаем процесс тренировки\n",
    "\n",
    "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уменьшение скорости обучения (learning rate decay)\n",
    "\n",
    "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
    "\n",
    "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
    "\n",
    "В нашем случае N будет равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.243613, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.298593, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.236525, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.212541, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.336710, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.237617, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.291851, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.247074, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.271449, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.266921, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.272612, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.224796, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.263617, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.298726, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.221359, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.303228, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.319364, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.346858, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.220475, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.292955, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Wall time: 2min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# DONE Implement learning rate decay inside Trainer.fit method\n",
    "# Decay should happen once per epoch\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate_decay=0.99)\n",
    "\n",
    "initial_learning_rate = trainer.learning_rate\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
    "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Накопление импульса (Momentum SGD)\n",
    "\n",
    "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
    "\n",
    "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
    "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
    "\n",
    "```\n",
    "velocity = momentum * velocity - learning_rate * gradient \n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
    "\n",
    "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
    "http://cs231n.github.io/neural-networks-3/#sgd  \n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.319482, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.306641, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.313926, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.313440, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.298050, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.265974, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302546, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.311003, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.334133, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.332669, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.281388, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.332419, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.295786, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.297263, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.261716, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.324117, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.288974, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.272671, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.251290, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.234552, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Wall time: 2min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# TODO: Implement MomentumSGD.update function in optim.py\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-4, learning_rate_decay=0.99)\n",
    "\n",
    "# You should see even better results than before!\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну что, давайте уже тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
    "\n",
    "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
    "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
    "\n",
    "Если этого не происходит, то где-то была допущена ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.339444, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.312066, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.317865, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.323978, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.277234, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.240588, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.293525, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.348892, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.250476, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.110080, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.158428, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.978957, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.183809, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.475926, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.330693, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.902110, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.030481, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.135387, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.632803, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.997377, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.116618, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.577501, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.342602, Train accuracy: 0.400000, val accuracy: 0.066667\n",
      "Loss: 1.915818, Train accuracy: 0.400000, val accuracy: 0.066667\n",
      "Loss: 1.776445, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.326349, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.280094, Train accuracy: 0.400000, val accuracy: 0.066667\n",
      "Loss: 1.218437, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.851130, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.705378, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.131983, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 1.635010, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 2.109653, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.969246, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.650840, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 2.033358, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.587459, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.056531, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.713991, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.737742, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.636943, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.885087, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.664881, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.799904, Train accuracy: 0.666667, val accuracy: 0.133333\n",
      "Loss: 0.949699, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.428387, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.614644, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.906414, Train accuracy: 0.733333, val accuracy: 0.133333\n",
      "Loss: 1.219015, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.521772, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.494944, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.319311, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.639918, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.476791, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.718600, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.489677, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.181399, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.041556, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.283690, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.824714, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.359035, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.614395, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.596687, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.353587, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 2.020191, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.375956, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 2.011360, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.607484, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.607932, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.691064, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.616778, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.331352, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.379066, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.232669, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.357323, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.328210, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.458794, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.261228, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.269219, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.959376, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.584070, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.897449, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.598947, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.682210, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.281119, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.251078, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.561577, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.259313, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.504664, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.532161, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.579963, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.533868, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.077958, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 0.967841, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.614174, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.572543, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.067023, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 0.990372, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.515819, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 0.988079, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.431975, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.323251, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.366172, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.611376, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.230167, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.610601, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.233376, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.503761, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.751648, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.360724, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.468304, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.280939, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.087435, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.115355, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.326569, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.369959, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.391751, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.226221, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.454094, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.476086, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.416705, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.278328, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.277106, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.101545, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.324465, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.110012, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.625384, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.105079, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.492194, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.132987, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.208204, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.337123, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.066690, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.179330, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.506226, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.116357, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.598203, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.166217, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.169326, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.436986, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.223732, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.193639, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.230147, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.496435, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.250413, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.223684, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.326787, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.279634, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.428066, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.264153, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Wall time: 6.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_size = 15\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=150, batch_size=5)\n",
    "\n",
    "# You should expect this to reach 1.0 training accuracy \n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
    "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
    "Найдите их!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.314988, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.298766, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.144087, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.242797, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.989847, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 1.803149, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.106706, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.839859, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 1.615912, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 1.676018, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 1.234868, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.307902, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.109053, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 0.496795, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.222246, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 0.564203, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 0.193415, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 0.825392, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 0.480197, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 0.438015, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Wall time: 756 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-3)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=3e-1, num_epochs=20, batch_size=5)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Итак, основное мероприятие!\n",
    "\n",
    "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
    "\n",
    "Добейтесь точности лучше **60%** на validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.301345, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.296685, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.295392, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.292667, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.292886, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.288086, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.292394, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.294554, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.297710, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.278861, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.282020, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.262619, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.269501, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.290668, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.309628, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.250897, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.278701, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.266558, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.283689, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.271987, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.254204, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.256777, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.274461, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.265826, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.237285, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.276781, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.258016, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.255436, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.271778, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.270737, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.268463, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.255610, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.287067, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.255636, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.258284, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.242891, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.261703, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.294536, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.220095, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.215548, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.223703, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.288817, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.197420, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.218544, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.205655, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229284, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229906, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.224056, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.268783, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.258319, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.266951, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.260582, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.260472, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.240448, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.293595, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.279499, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.273833, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.178586, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.261834, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.274796, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.273060, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.205315, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.163351, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.246701, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.208030, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.143228, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.262605, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.128938, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.266845, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.231137, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.213915, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.161845, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.269579, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.117874, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.251191, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.214249, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.219777, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.183124, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.262795, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229928, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.216947, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230809, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.239159, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.213408, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227393, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.345262, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.264797, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.273405, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.157646, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.196325, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.253617, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.224037, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.185363, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.168969, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.207576, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.242930, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.212178, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.314609, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.253745, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.197384, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.236137, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.257337, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230955, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.177808, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.172800, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.256288, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.285010, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.207515, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.238238, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.275141, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.250915, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.117299, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.261489, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.239052, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.292589, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.201873, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.255431, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.231940, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.213310, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.214249, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.223907, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.311860, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.207406, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.081252, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.221303, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.212796, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.217240, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.350492, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.147561, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.173892, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.131117, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.248074, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.209344, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.314771, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.267229, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.181215, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.125028, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.201831, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.281480, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.195822, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.274001, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.216008, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.280440, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.216886, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.253273, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.157959, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.103144, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.257199, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.192811, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.216028, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.318989, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.153227, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.199905, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.221789, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.207210, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.198195, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.211818, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.189895, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.153280, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229798, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230630, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.280964, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.167806, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229709, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.102329, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.285130, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.279950, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.207111, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.175689, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.143391, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.271161, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230301, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.298217, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.162123, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227675, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.289964, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.220734, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.143948, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.272166, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.129017, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.214847, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.225560, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.235965, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.211325, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.285692, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.270288, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.267070, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.308875, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.225575, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.186600, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.239929, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.111405, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.105046, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.166791, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.264966, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.264561, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.116708, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.189100, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.203388, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.227134, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Wall time: 14min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Let's train the best one-hidden-layer network we can\n",
    "\n",
    "learning_rates = 1e-4\n",
    "reg_strength = 1e-3\n",
    "learning_rate_decay = 0.999\n",
    "hidden_layer_size = 128\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "loss_history = []\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 128, reg = 1e-3)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "# trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-4, learning_rate_decay=0.999)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-4, learning_rate_decay=0.999, num_epochs=200, batch_size=64)\n",
    "\n",
    "# TODO find the best hyperparameters to train the network\n",
    "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
    "# You should expect to get to at least 40% of valudation accuracy\n",
    "# Save loss/train/history of the best classifier to the variables above\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "# print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1aabb3d0>]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.subplot(212)\n",
    "plt.title(\"Train/validation accuracy\")\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural net test set accuracy: 0.182000\n",
      "Wall time: 47 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_pred = model.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
